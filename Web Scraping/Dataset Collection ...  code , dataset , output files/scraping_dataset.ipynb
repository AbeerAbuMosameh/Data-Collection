{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20efa141-9035-41ec-bda2-26ece57fc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # use it for send request to page url that i want scrapping data from\n",
    "from bs4 import BeautifulSoup  # use it in web scraping process 'parsing HTML content'\n",
    "import pandas as pd \n",
    "import time # add delays between requests to avoid blocke\n",
    "import random  # use it to add delays between requests \"to not blocked from site\"\n",
    "from fake_useragent import UserAgent  # used to to generate random user agents for each request .. same user agent get 503 can send req only one\n",
    "import re  # used for regular expressions -  some data shoud extacted from specif expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ce56ca-24ab-42e6-a216-31398d56bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the soup object \"to parse\" from the URL\n",
    "def get_soup(url, headers):\n",
    "    for _ in range(5):  # Retry up to 5 times if the request fails\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)  # sedn GET HTTP request to the URL\n",
    "            response.raise_for_status()  # raise an HTTP Error for bad responses 400 and 500 \"error has occurred during the process\"\n",
    "            return BeautifulSoup(response.content, 'html.parser')  # parse HTML content to use it and extract data\n",
    "        except requests.RequestException as e:\n",
    "            # print(f\"Failed to retrieve page {url}, error: {e}\")  # error message\n",
    "            time.sleep(random.randint(1, 5))  # Wwit for a random time between 1 to 5 seconds before retrying\n",
    "    return None  # Return None if the request fails after 5 retries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f965c89-0fc3-493a-9072-fe943ff85fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_product(product, page_number,category):\n",
    "    def safe_find(element, search_dict, text=False):\n",
    "        try:\n",
    "            found = element.find(**search_dict)  # search for the element using provided criteria \"clas , attribute\"\n",
    "            return found.text.strip() if (found and text) else found  # text if text=True, else return element itself\n",
    "        except AttributeError:\n",
    "            return None  # if element not found\n",
    "\n",
    "    # extract product name ( product to search in ,  criteria to search based on , test =True)\n",
    "    name = safe_find(product, {'name': 'span', 'attrs': {'class': 'a-size-medium a-color-base a-text-normal'}}, text=True)\n",
    "    if not name:\n",
    "        name = safe_find(product, {'name': 'div', 'attrs': {'data-cy': 'title-recipe'}}, text=True)\n",
    "    \n",
    "    # extract product price\n",
    "    price = safe_find(product, {'name': 'span', 'attrs': {'class': 'a-offscreen'}}, text=True)\n",
    "    if not price:\n",
    "        price = safe_find(product, {'name': 'div', 'attrs': {'class': 'a-row a-size-base a-color-secondary'}}, text=True)\n",
    "    \n",
    "    # extract src of product image\n",
    "    image_element = safe_find(product, {'name': 'img', 'attrs': {'class': 's-image'}})\n",
    "    image = image_element['src'] if image_element else None\n",
    "\n",
    "    # extract product rating\n",
    "    rating_text = safe_find(product, {'name': 'span', 'attrs': {'class': 'a-icon-alt'}}, text=True)\n",
    "    rating = rating_text.split()[0] if rating_text else None\n",
    "    \n",
    "    # extract rating count\n",
    "    try:\n",
    "        rating_count_element = product.find(\"div\", {\"class\": \"s-csa-instrumentation-wrapper\"}).find(\"span\", {\"aria-label\": True})\n",
    "        rating_count_text = rating_count_element.text.strip() if rating_count_element else None\n",
    "        rating_count = re.sub(r'[^0-9]', '', rating_count_text) if rating_count_text else None\n",
    "    except AttributeError:\n",
    "        rating_count = None\n",
    "    \n",
    "    # extract delivery information\n",
    "    try:\n",
    "        delivery_element = product.find(\"div\", {\"data-cy\": \"delivery-recipe\"})\n",
    "        delivery = delivery_element.find(\"span\", {\"aria-label\": True}).text.strip() if delivery_element else None\n",
    "    except AttributeError:\n",
    "        delivery = safe_find(product, {'name': 'span', 'attrs': {'aria-label': True}}, text=True)\n",
    "    \n",
    "    # is the product is a \"Best Seller\"\n",
    "    best_seller_element = safe_find(product, {'name': 'span', 'attrs': {'class': 'a-badge-text'}}, text=True)\n",
    "    is_best_seller = 1 if best_seller_element and \"Best Seller\" in best_seller_element else 0\n",
    "\n",
    "    # check if the product is an \"Overall Pick\" ..  Products highlighted as 'Overall Pick' are: Rated 4+ stars , Purchased often ,Returned infrequently\n",
    "    overall_pick_element = safe_find(product, {'name': 'span', 'attrs': {'class': 'a-badge-text', 'data-a-badge-color': 'sx-cloud'}}, text=True)\n",
    "    is_overall_pick = 1 if overall_pick_element and \"Overall Pick\" in overall_pick_element else 0\n",
    "\n",
    "    # Return product details\n",
    "    return {\n",
    "        'page': page_number,\n",
    "        'name': name,\n",
    "        'category': category,\n",
    "        'image': image,\n",
    "        'price': price,\n",
    "        'rating': rating,\n",
    "        'rating_count': rating_count,\n",
    "        'delivery': delivery,\n",
    "        'is_best_seller': is_best_seller,\n",
    "        'is_overall_pick': is_overall_pick\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3f5800-264b-4d5b-b781-2c9dcc15ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape each page \"url\"\n",
    "def scrape_page(url, headers, page_number, category):\n",
    "    soup = get_soup(url, headers)  # html to parse it and extract data \n",
    "    if not soup:\n",
    "        return []\n",
    "\n",
    "    # extract product divs that contain the required data-component-type attribute\n",
    "    product_divs = soup.find_all('div', {\"data-component-type\": \"s-search-result\"})\n",
    "    # Print the product divs to inspect their structure \n",
    "    #for index, product_div in enumerate(product_divs): \n",
    "    #   print(f\"Product div {index}:\\n\", product_div.prettify(), \"\\n **************************\\n\") #get what this piece of code does\n",
    "    \n",
    "    # parse each product in the product divs\n",
    "    products = [parse_product(product, page_number, category) for product in product_divs]\n",
    "\n",
    "    return [product for product in products if product]  # filter None values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8ecb6a-e23e-4b31-b444-0aeea39727ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()  #  UserAgent for random headers - one for each req\n",
    "\n",
    "headers = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",  # accept-language header\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",  #accept-encoding header\n",
    "    \"User-Agent\": ua.random,  # random User-Agent for each request\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"Connection\": \"close\"  # close connection after each request to avoid block\n",
    "}\n",
    "\n",
    "categories = ['electronics', 'toys', 'mens', 'womens', 'foods', 'clothes', 'printers', 'flowers', 'accessories']\n",
    "all_products = []\n",
    "\n",
    "# for each category\n",
    "for category in categories:\n",
    "    page_number = 1  # reset page number for each category\n",
    "\n",
    "    # 10 pages for each category\n",
    "    while page_number < 10:  # cant do this to be 404 becouse if page number not found display last avaliable so no condition to stop loop and huge data\n",
    "        headers['User-Agent'] = ua.random  # new random User-Agent for each requ\n",
    "        url = f'https://www.amazon.com/s?k={category}&page={page_number}&_encoding=UTF8&content-id=amzn1.sym.ce070039-db53-47a0-8017-250744e811c9&pd_rd_r=ed6d9351-0c78-4ace-9b09-c8b2afa75bf4&pd_rd_w=eiNsv&pd_rd_wg=mZPMW&pf_rd_p=ce070039-db53-47a0-8017-250744e811c9&pf_rd_r=WC3A9Q78YZWRRZ781RQE&qid=1732179514&ref=sr_pg_{page_number}'  # Construct the URL for the current page\n",
    "        products = scrape_page(url, headers, page_number, category)  # get product data\n",
    "\n",
    "        if not products:\n",
    "            break\n",
    "\n",
    "        all_products.extend(products)  # add the products to the list\n",
    "        page_number += 1  # next page \"iteration\"\n",
    "\n",
    "        # delay to avoid block by site\n",
    "        time.sleep(random.randint(1, 5))\n",
    "\n",
    "# convert all products to a DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "df.index += 1  # index from 1\n",
    "df.to_csv('amazon_products_final.csv', index_label='index')  # Save the DataFrame to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a8c220-3b2c-44a1-b6e8-18e85f9f96ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2015 entries, 0 to 2014\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   index            2015 non-null   int64  \n",
      " 1   page             2015 non-null   int64  \n",
      " 2   name             2015 non-null   object \n",
      " 3   category         2015 non-null   object \n",
      " 4   image            2015 non-null   object \n",
      " 5   price            1950 non-null   object \n",
      " 6   rating           1849 non-null   float64\n",
      " 7   rating_count     1782 non-null   float64\n",
      " 8   delivery         1782 non-null   object \n",
      " 9   is_best_seller   2015 non-null   int64  \n",
      " 10  is_overall_pick  2015 non-null   int64  \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 173.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('amazon_products_final.csv')\n",
    "data.info() #info about scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0e02a-bf6a-4d2c-8692-4d6bc79863d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
